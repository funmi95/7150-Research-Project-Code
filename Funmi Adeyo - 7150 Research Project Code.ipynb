{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from time import time\n",
    "from sklearn.model_selection import KFold\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\user\\Documents\\Degrees\\Data Science MSc\\8. 7150CEM Data Science Project (50 credits)')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list that will be the collumn names for the PPD and read the PPD datatsets in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['tui', 'price', 'DoT',\"POSTCODE\", 'propertyType', 'Age','Duration',\n",
    "         'PAON', 'SAON', 'street', 'Locality', 'TownCity', 'District', 'Country',\n",
    "         'ppdCat', 'RecStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Pandas to read in the 2020 PPD\n",
    "pp2020Data = pd.read_csv('price_paid_2020.csv',header=None, names=names)\n",
    "\n",
    "#Use dask to read in the full 1995-2021 PPD\n",
    "ppFullData = dd.read_csv('price_paid_complete.csv',header=None, names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITERATION/SUBSET 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat a Coventry subset of the 2020 PPD (SUBSET 1)\n",
    "CoventryPP2020 = pp2020Data[pp2020Data.TownCity == 'COVENTRY']\n",
    "len(CoventryPP2020) #4357\n",
    "#To make a column with full addresses. \n",
    "CoventryPP2020['FullAddress'] = CoventryPP2020[['PAON', 'SAON', 'street']].astype(str).agg(','.join, axis=1)\n",
    "CoventryPP2020['FullAddress'].replace({'nan,':' '}, regex=True, inplace = True)\n",
    "\n",
    "#To make an EPC dataset for Coventry properties \n",
    "CoventryEPC2020 = pd.read_csv(r'all-domestic-certificates\\domestic-E08000026-Coventry\\certificates.csv')\n",
    "len(CoventryEPC2020) #\n",
    "#Upper case EPC\n",
    "CoventryEPC2020['ADDRESS'] = CoventryEPC2020['ADDRESS'].str.upper()\n",
    "\n",
    "#To merge the datasets\n",
    "MergedData = CoventryPP2020.merge(CoventryEPC2020, how='inner', left_on=['FullAddress','POSTCODE'], right_on=['ADDRESS','POSTCODE'])\n",
    "len(MergedData) #4030\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITERATION/SUBSET 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat a 2020 Subset with the largest cities (SUBSET 2)\n",
    "ManchesterPP2020 = pp2020Data[pp2020Data.TownCity == 'MANCHESTER']\n",
    "len(ManchesterPP2020) #13936\n",
    "#Selecting Reading\n",
    "NothamPP2020 = pp2020Data[pp2020Data.TownCity == 'NOTTINGHAM']\n",
    "len(NothamPP2020) #10914\n",
    "#Selecting Birmingham\n",
    "BhamPP2020 = pp2020Data[pp2020Data.TownCity == 'BIRMINGHAM']\n",
    "len(BhamPP2020) #11492\n",
    "#Selecting Cardif\n",
    "BristolPP2020 = pp2020Data[pp2020Data.TownCity == 'BRISTOL']\n",
    "len(BristolPP2020) #11745\n",
    "#Selecting Liverpool\n",
    "LiverpoolPP2020 = pp2020Data[pp2020Data.TownCity == 'LIVERPOOL']\n",
    "len(LiverpoolPP2020) #9776\n",
    "#Selecting Leeds\n",
    "LeedsPP2020 = pp2020Data[pp2020Data.TownCity == 'LEEDS']\n",
    "len(LeedsPP2020) #9479\n",
    "#Selecting Birmingham\n",
    "SheffieldPP2020 = pp2020Data[pp2020Data.TownCity == 'SHEFFIELD']\n",
    "len(SheffieldPP2020) #8183\n",
    "#Selecting Leicester\n",
    "LeicesterPP2020 = pp2020Data[pp2020Data.TownCity == 'LEICESTER']\n",
    "len(LeicesterPP2020) #6927\n",
    "\n",
    "#To merge the PP data for all 9 cities\n",
    "PPframes = [CoventryPP2020, ManchesterPP2020, NothamPP2020, BhamPP2020, \n",
    "            BristolPP2020,LiverpoolPP2020, LeedsPP2020, SheffieldPP2020, \n",
    "            LeicesterPP2020]\n",
    "PP_9_cities = pd.concat(PPframes)\n",
    "len(PP_9_cities) #86809\n",
    "\n",
    "#To make a column with full addresses. \n",
    "PP_9_cities['FullAddress'] = PP_9_cities[['PAON', 'SAON', 'street']].astype(str).agg(','.join, axis=1)\n",
    "PP_9_cities['FullAddress'].replace({'nan,':' '}, regex=True, inplace = True)\n",
    "\n",
    "##Making EPC dataframe\n",
    "#To make an EPC dataset for Coventry properties \n",
    "CoventryEPC = pd.read_csv(r'all-domestic-certificates\\domestic-E08000026-Coventry\\certificates.csv')\n",
    "len(CoventryEPC) #125,481\n",
    "\n",
    "#To make an EPC dataset for Manchester properties \n",
    "ManchesterEPC = pd.read_csv(r'all-domestic-certificates\\domestic-E08000003-Manchester\\certificates.csv')\n",
    "len(ManchesterEPC) #245,779\n",
    "\n",
    "#To make an EPC dataset for Nottingham properties \n",
    "NothamEPC = pd.read_csv(r'all-domestic-certificates/domestic-E06000018-Nottingham\\certificates.csv')\n",
    "len(NothamEPC) #161837\n",
    "\n",
    "#To make an EPC dataset for Birmingham properties \n",
    "BhamEPC = pd.read_csv(r'all-domestic-certificates/domestic-E08000025-Birmingham\\certificates.csv')\n",
    "len(BhamEPC) #393335\n",
    "\n",
    "#To make an EPC dataset for Bristol properties \n",
    "BristolEPC = pd.read_csv(r'all-domestic-certificates/domestic-E06000023-Bristol-City-of\\certificates.csv')\n",
    "len(BristolEPC) #173272\n",
    "\n",
    "#To make an EPC dataset for Liverpool properties \n",
    "LiverpoolEPC = pd.read_csv(r'all-domestic-certificates/domestic-E08000012-Liverpool\\certificates.csv')\n",
    "len(LiverpoolEPC) #201791\n",
    "\n",
    "#To make an EPC dataset for Leeds properties \n",
    "LeedsEPC = pd.read_csv(r'all-domestic-certificates/domestic-E08000035-Leeds\\certificates.csv')\n",
    "len(LeedsEPC) #315050\n",
    "\n",
    "#To make an EPC dataset for Sheffield properties \n",
    "SheffieldEPC = pd.read_csv(r'all-domestic-certificates/domestic-E08000019-Sheffield\\certificates.csv')\n",
    "len(SheffieldEPC) #192783\n",
    "\n",
    "#To make an EPC dataset for Leicester properties \n",
    "LeicesterEPC = pd.read_csv(r'all-domestic-certificates/domestic-E06000016-Leicester\\certificates.csv')\n",
    "len(LeicesterEPC) #124361\n",
    "\n",
    "#To merge the EPC data for all 9 cities\n",
    "EPCframes = [CoventryEPC, ManchesterEPC, NothamEPC, BhamEPC, BristolEPC,\n",
    "             LiverpoolEPC, LeedsEPC, SheffieldEPC, LeicesterEPC]\n",
    "EPC_9_cities = pd.concat(EPCframes)\n",
    "#To make the address upper case as the address in the PP data are uppercase too\n",
    "EPC_9_cities['ADDRESS'] = EPC_9_cities['ADDRESS'].str.upper()\n",
    "len(EPC_9_cities) #1933689\n",
    "\n",
    "MergedData = PP_9_cities.merge(EPC_9_cities, how='inner', left_on=['FullAddress','POSTCODE'], right_on=['ADDRESS','POSTCODE'])\n",
    "len(MergedData)\n",
    "#45053\n",
    "\n",
    "#Quick Read-in for subset 2\n",
    "#NoDupData = pd.read_csv('cities_9_PP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITERATION/SUBSET 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat a Coventry Subset with all the data from 1995-2021\n",
    "#Create a Coventry subset\n",
    "CoventryPP = ppFullData[ppFullData.TownCity == 'COVENTRY'].compute()\n",
    "len(CoventryPP)\n",
    "\n",
    "#####CREATE CSV FOR QUICK LOADING IN FUTURE\n",
    "CoventryPP.to_csv('CoventryPricePaid.csv',index=False)\n",
    "\n",
    "#Quick Read in of CoventryPP\n",
    "CoventryPP = pd.read_csv('CoventryPricePaid.csv')\n",
    "len(CoventryPP) #151761\n",
    "\n",
    "#To make a column with full addresses to match that found in the EPC\n",
    "CoventryPP['FullAddress'] = CoventryPP[['PAON', 'SAON', 'street']].astype(str).agg(','.join, axis=1)\n",
    "CoventryPP['FullAddress'].replace({'nan,':' '}, regex=True, inplace = True)\n",
    "\n",
    "\n",
    "#CoventryEPC\n",
    "CoventryEPC = pd.read_csv(r'all-domestic-certificates\\domestic-E08000026-Coventry\\certificates.csv')\n",
    "len(CoventryEPC) #125481\n",
    "#Upper case the address to match the way it is in the PPD\n",
    "CoventryEPC['ADDRESS'] = CoventryEPC['ADDRESS'].str.upper()\n",
    "\n",
    "#Merge the Coventry PPD and EPC based on address and postcode\n",
    "MergedData = CoventryPP.merge(CoventryEPC, how='inner', left_on=['FullAddress','POSTCODE'], right_on=['ADDRESS','POSTCODE'])\n",
    "len(MergedData) #109997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL SUBSETS USE THIS CODE TO REMOVE DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Duplicates by keeping only the latest EPC Inspection\n",
    "MergedData[\"INSPECTION_DATE\"] = pd.to_datetime(MergedData[\"INSPECTION_DATE\"] )\n",
    "NoDupData = MergedData.sort_values(by=\"INSPECTION_DATE\")\n",
    "NoDupData.drop_duplicates(subset=['BUILDING_REFERENCE_NUMBER'], keep='last', inplace=True)\n",
    "len(NoDupData) #45697\n",
    "\n",
    "\n",
    "#Create year column (For subset 3 only)\n",
    "NoDupData['Year'] = pd.DatetimeIndex(NoDupData['DoT']).year\n",
    "\n",
    "#### USE FOR ITERATION 1 AND 2 ###\n",
    "UsefulData = NoDupData[['price','DoT','POSTCODE', 'FullAddress', 'TownCity',\n",
    "                        'BUILDING_REFERENCE_NUMBER','propertyType','Age','Duration',\n",
    "                        'CURRENT_ENERGY_RATING', 'CURRENT_ENERGY_EFFICIENCY',\n",
    "                        'PROPERTY_TYPE', 'BUILT_FORM', 'ENERGY_CONSUMPTION_CURRENT',\n",
    "                        'LIGHTING_COST_CURRENT', 'HEATING_COST_CURRENT',\n",
    "                        'HOT_WATER_COST_CURRENT','TOTAL_FLOOR_AREA', 'EXTENSION_COUNT',\n",
    "                        'NUMBER_HABITABLE_ROOMS','CONSTRUCTION_AGE_BAND','FLOOR_HEIGHT']]\n",
    "\n",
    "\n",
    "\n",
    "#### USE FOR ITERATION 3 ###\n",
    "UsefulData = NoDupData[['price','DoT','POSTCODE', 'FullAddress', 'TownCity',\n",
    "                        'BUILDING_REFERENCE_NUMBER','propertyType','Age','Duration',\n",
    "                        'CURRENT_ENERGY_RATING', 'CURRENT_ENERGY_EFFICIENCY',\n",
    "                        'PROPERTY_TYPE', 'BUILT_FORM', 'ENERGY_CONSUMPTION_CURRENT',\n",
    "                        'LIGHTING_COST_CURRENT', 'HEATING_COST_CURRENT',\n",
    "                        'HOT_WATER_COST_CURRENT','TOTAL_FLOOR_AREA', 'EXTENSION_COUNT',\n",
    "                        'NUMBER_HABITABLE_ROOMS','CONSTRUCTION_AGE_BAND','FLOOR_HEIGHT',\n",
    "                        'Year']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MISSING VALUES ###\n",
    "UsefulData.info()\n",
    "#Check missing values\n",
    "UsefulData.isnull().sum()\n",
    "#EXTENSION_COUNT               4356\n",
    "#NUMBER_HABITABLE_ROOMS        4356\n",
    "#CONSTRUCTION_AGE_BAND          191\n",
    "UsefulData.isna().sum()\n",
    "#Check the % of missing values\n",
    "round(UsefulData.isnull().sum()*100/UsefulData.shape[0],2).sort_values(ascending=False).head(20)\n",
    "#Drop the rows where there are missing values in number of habitable rooms\n",
    "#These are the same properties where extension count and construction age band \n",
    "#information was missing\n",
    "UsefulData.dropna(how='any',subset=['NUMBER_HABITABLE_ROOMS'], inplace = True)\n",
    "UsefulData.dropna(how='any',subset=['CONSTRUCTION_AGE_BAND'], inplace = True)\n",
    "\n",
    "#Most properties do not have information for floor height so this column can be dropped completely\n",
    "\n",
    "UsefulData.Duration.value_counts()\n",
    "#Relates to the tenure: F = Freehold, L= Leasehold etc.\n",
    "#F    37409\n",
    "#L     3734\n",
    "#U        3\n",
    "#Will remove the 3 U as these are considered missing values\n",
    "UsefulData = UsefulData[UsefulData.Duration != 'U']\n",
    "len(UsefulData) #41173\n",
    "\n",
    "UsefulData.propertyType.value_counts()\n",
    "#D = Detached, S = Semi-Detached, T = Terraced, F = Flats/Maisonettes, O = Other\n",
    "#T    24061\n",
    "#S    10340\n",
    "#D     3346\n",
    "#F     3331\n",
    "#O       98\n",
    "\n",
    "\n",
    "UsefulData.PROPERTY_TYPE.value_counts()\n",
    "#House         35969\n",
    "#Flat           2856\n",
    "#Bungalow       1485\n",
    "#Maisonette      866\n",
    "\n",
    "UsefulData.BUILT_FORM.value_counts()\n",
    "#Together with the PROPERTY TYPE from EPC, the Build Form produces a structured \n",
    "#description of the property. So will drop property type from price paid\n",
    "#Mid-Terrace             17006\n",
    "#Semi-Detached           11636\n",
    "#End-Terrace              8134\n",
    "#Detached                 3998\n",
    "#Enclosed End-Terrace      208\n",
    "#NO DATA!                  114\n",
    "#Enclosed Mid-Terrace       80\n",
    "#will remove the rows where there is no data if keeping the built_form variable\n",
    "#UsefulData = UsefulData[UsefulData.BUILT_FORM != 'NO DATA!']\n",
    "\n",
    "UsefulData.Age.value_counts()\n",
    "#Y = a newly built property, N = an established residential building\n",
    "#N    39553\n",
    "#Y     1623\n",
    "\n",
    "UsefulData.CONSTRUCTION_AGE_BAND.value_counts()\n",
    "#Construction age band columns from the EPC dataset is much more nuanced than \n",
    "#the old/new (age) column in the price paid dataset. So will drop the Age from PP data\n",
    "\n",
    "#England and Wales: 1930-1949       12568\n",
    "#England and Wales: 1900-1929        9574\n",
    "#England and Wales: 1950-1966        8541\n",
    "#England and Wales: 1967-1975        3442\n",
    "#England and Wales: before 1900      1371\n",
    "#England and Wales: 1996-2002        1199\n",
    "#England and Wales: 2003-2006        1152\n",
    "#England and Wales: 1983-1990        1048\n",
    "#England and Wales: 1976-1982        1016\n",
    "#England and Wales: 2007 onwards      634\n",
    "#England and Wales: 1991-1995         601\n",
    "#INVALID!                              29\n",
    "#England and Wales: 2007-2011           1\n",
    "#will remove invalid rows as missing values as well as the row where there is only 1 oveservation\n",
    "UsefulData = UsefulData[UsefulData.CONSTRUCTION_AGE_BAND != 'INVALID!']\n",
    "UsefulData = UsefulData[UsefulData.CONSTRUCTION_AGE_BAND != 'England and Wales: 2007-2011']\n",
    "len(UsefulData) #41143\n",
    "\n",
    "\n",
    "UsefulData.NUMBER_HABITABLE_ROOMS.value_counts()\n",
    "#includes kitchen and living room\n",
    "#5.0     14524\n",
    "#4.0     13199\n",
    "#3.0      5569\n",
    "#6.0      4098\n",
    "#7.0      1667\n",
    "#2.0       827\n",
    "#8.0       677\n",
    "#9.0       303\n",
    "#10.0      132\n",
    "#1.0        58\n",
    "#11.0       42\n",
    "#12.0       21\n",
    "#14.0        8\n",
    "#13.0        6\n",
    "#15.0        4\n",
    "#16.0        4\n",
    "#20.0        2\n",
    "#22.0        1\n",
    "#17.0        1\n",
    "#Exclude properties with >13 rooms as there are so few of them\n",
    "UsefulData = UsefulData[UsefulData.NUMBER_HABITABLE_ROOMS < 13]\n",
    "len(UsefulData)#41117\n",
    "\n",
    "UsefulData.CURRENT_ENERGY_RATING.value_counts()\n",
    "#D    22383\n",
    "#E     8568\n",
    "#C     8165\n",
    "#F     1204\n",
    "#B      472\n",
    "#G      302\n",
    "#A       23\n",
    "\n",
    "UsefulData.EXTENSION_COUNT.value_counts()\n",
    "#0.0    20290\n",
    "#1.0    16346\n",
    "#2.0     3884\n",
    "#3.0      494\n",
    "#4.0      103\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers for subset 1&2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UsefulData.price.describe()\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data = UsefulData, y = 'price')\n",
    "plt.xlabel('All properties', fontsize = 10)\n",
    "plt.ylabel('Price', fontsize = 10)\n",
    "plt.title('Boxplot for Price Before Removing Outliers')\n",
    "plt.show()\n",
    "\n",
    "Q1=UsefulData['price'].quantile(0.25)\n",
    "Q3=UsefulData['price'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "Lower_Whisker = Q1-1.5*IQR\n",
    "Upper_Whisker = Q3+1.5*IQR\n",
    "print(Lower_Whisker, Upper_Whisker)\n",
    "\n",
    "UsefulData = UsefulData[UsefulData['price']< Upper_Whisker]\n",
    "UsefulData = UsefulData[UsefulData['price']> Lower_Whisker]\n",
    "sns.boxplot(data = UsefulData, y = 'price')\n",
    "UsefulData.price.describe()\n",
    "len(UsefulData) #28352\n",
    "\n",
    "\n",
    "sns.distplot(UsefulData['price'])\n",
    "sns.boxplot(data = UsefulData, y = 'price')\n",
    "UsefulData.price.describe()\n",
    "\n",
    "min(UsefulData.price)\n",
    "\n",
    "UsefulData.price\n",
    "UsefulData.shape # There are now 6156 rows (was 6469 before so lost 313 rows)\n",
    "\n",
    "#2 OUTLIERS IN FLOOR AREA\n",
    "UsefulData.TOTAL_FLOOR_AREA.describe()\n",
    "sns.boxplot(data = UsefulData, y = 'TOTAL_FLOOR_AREA')\n",
    "sns.distplot(UsefulData['TOTAL_FLOOR_AREA'])\n",
    "\n",
    "UsefulData = UsefulData[UsefulData.TOTAL_FLOOR_AREA < 150]\n",
    "UsefulData = UsefulData[UsefulData['TOTAL_FLOOR_AREA'] >30]\n",
    "\n",
    "#Looking for the upper & lower whisker as well as the interquartile range\n",
    "Q1=UsefulData['TOTAL_FLOOR_AREA'].quantile(0.25)\n",
    "Q3=UsefulData['TOTAL_FLOOR_AREA'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "Lower_Whisker = Q1-1.5*IQR\n",
    "Upper_Whisker = Q3+1.5*IQR\n",
    "print(Lower_Whisker, Upper_Whisker)\n",
    "\n",
    "UsefulData = UsefulData[UsefulData['TOTAL_FLOOR_AREA']< Upper_Whisker]\n",
    "UsefulData = UsefulData[UsefulData['TOTAL_FLOOR_AREA']> Lower_Whisker]\n",
    "\n",
    "sns.distplot(UsefulData['TOTAL_FLOOR_AREA'])\n",
    "sns.boxplot(data = UsefulData, y = 'TOTAL_FLOOR_AREA')\n",
    "UsefulData.TOTAL_FLOOR_AREA.describe()\n",
    "UsefulData.shape \n",
    "\n",
    "#2454 for subset 1\n",
    "#27341 for 9 subset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers for subset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UsefulData.price.describe()\n",
    "\n",
    "#Reveals that max price is 3.174837e+06\n",
    "#min price is 7.250000e+02\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data = UsefulData, y = 'price')\n",
    "plt.xlabel('All properties', fontsize = 10)\n",
    "plt.ylabel('Price', fontsize = 10)\n",
    "plt.title('Boxplot for Price Before Removing Outliers')\n",
    "plt.show()\n",
    "\n",
    "#Get rid of extremes (properties over £600,000 and less than 10,000)\n",
    "NoOutlieData = UsefulData[UsefulData.price < 1500000]\n",
    "\n",
    "#Now look at boxplot\n",
    "plt.figure()\n",
    "plt.xlabel('Price', fontsize = 10)\n",
    "plt.ylabel('Density', fontsize = 10)\n",
    "plt.title('Distribution of Price')\n",
    "sns.distplot(NoOutlieData['price'])\n",
    "plt.show()\n",
    "\n",
    "len(NoOutlieData) #40913\n",
    "\n",
    "#Boxplot to show price by year\n",
    "plt.figure()\n",
    "plt.xlabel('Year', fontsize = 10)\n",
    "plt.ylabel('Price', fontsize = 10)\n",
    "#plt.title('Price Each Year Before Removing Outliers')\n",
    "ax = sns.boxplot(data = NoOutlieData, x='Year',y='price')\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()\n",
    "\n",
    "#Displot to show that price is not normally distributed and therefore the \n",
    "#interquartile range method should be used to remove outliers.\n",
    "sns.distplot(NoOutlieData['price'])\n",
    "\n",
    "#Create functions to address outliers in price\n",
    "def OutlierForYear(NoOutlieData, year):\n",
    "    \"\"\"Function to remove the rows with outliers in price\n",
    "    for a given year. Outliers removed using the interquartile\n",
    "    range method.\"\"\"\n",
    "    frame = NoOutlieData[NoOutlieData.Year == year]\n",
    "    Q1=frame['price'].quantile(0.25)\n",
    "    Q3=frame['price'].quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    #print(Q1)\n",
    "    #print(Q3)\n",
    "    #print(IQR)\n",
    "    Lower_Whisker = Q1-1.5*IQR\n",
    "    Upper_Whisker = Q3+1.5*IQR\n",
    "    #print(Lower_Whisker, Upper_Whisker)\n",
    "    frame = frame[frame['price']< Upper_Whisker]\n",
    "    frame = frame[frame['price']> Lower_Whisker]\n",
    "    #sns.boxplot(data = frame, y = 'price')\n",
    "    frame.price.describe()\n",
    "    #print(len(frame)) #39192\n",
    "    return(frame)\n",
    "\n",
    "def All_Outliers():\n",
    "    \"\"\"Function to apply the outlier removal function to each year in the \n",
    "    PPD & EPC merged dataset.\"\"\"\n",
    "    OutliersGone=pd.DataFrame()\n",
    "    for year in NoOutlieData.Year.unique():\n",
    "        frame = OutlierForYear(NoOutlieData, year)\n",
    "        OutliersGone=OutliersGone.append(frame,ignore_index=True)\n",
    "    return(OutliersGone)\n",
    "\n",
    "#Apply the function    \n",
    "Outlier_Free = All_Outliers()\n",
    "len(Outlier_Free) #38452\n",
    "\n",
    "#Boxplot to show price by year after removing outliers\n",
    "ax = sns.boxplot(data = Outlier_Free, x='Year',y='price')\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "    \n",
    "Outlier_Free.price.describe()\n",
    "\n",
    "#Now address outliers in total floor area\n",
    "Outlier_Free.TOTAL_FLOOR_AREA.describe()\n",
    "#Boxplot in total floor are BEFORE removing outliers\n",
    "sns.boxplot(data = Outlier_Free, y = 'TOTAL_FLOOR_AREA')\n",
    "#Displot shows it is not evenly distributed either.\n",
    "sns.distplot(Outlier_Free['TOTAL_FLOOR_AREA'])\n",
    "\n",
    "#Use the interquartile range method to remove outliers.  \n",
    "Q1=Outlier_Free['TOTAL_FLOOR_AREA'].quantile(0.25)\n",
    "Q3=Outlier_Free['TOTAL_FLOOR_AREA'].quantile(0.75)\n",
    "IQR=Q3-Q1\n",
    "print(Q1)\n",
    "print(Q3)\n",
    "print(IQR)\n",
    "Lower_Whisker = Q1-1.5*IQR\n",
    "Upper_Whisker = Q3+1.5*IQR\n",
    "print(Lower_Whisker, Upper_Whisker)\n",
    "\n",
    "Outlier_Free = Outlier_Free[Outlier_Free['TOTAL_FLOOR_AREA']< Upper_Whisker]\n",
    "Outlier_Free = Outlier_Free[Outlier_Free['TOTAL_FLOOR_AREA']> Lower_Whisker]\n",
    "\n",
    "#Boxplot for total floor area after removing outliers\n",
    "sns.boxplot(data = Outlier_Free, y = 'TOTAL_FLOOR_AREA')\n",
    "Outlier_Free.TOTAL_FLOOR_AREA.describe()\n",
    "Outlier_Free.shape # 37086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploratory Data Analysis of Categorical Features After Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For property type\n",
    "Outlier_Free['propertyType']=Outlier_Free['propertyType'].map({'F': 'Flat/Maissonette', \n",
    "                                                               'T': 'Terraced',\n",
    "                                                               'O': 'Other',\n",
    "                                                               'D': 'Detached',\n",
    "                                                               'S': 'Semi-detached'})\n",
    "#D = Detached, S = Semi-Detached, T = Terraced, F = Flats/Maisonettes, O = Other\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.boxplot(data = Outlier_Free, x='propertyType',y='price')\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()\n",
    "\n",
    "#For energy rating\n",
    "plt.figure()\n",
    "ax = sns.boxplot(data = Outlier_Free, x='CURRENT_ENERGY_RATING',y='price')\n",
    "plt.show()\n",
    "\n",
    "#For duration\n",
    "Outlier_Free['Duration']=Outlier_Free['Duration'].map({'F': 'Freehold', 'L': 'Leasehold'})\n",
    "plt.figure()\n",
    "ax = sns.boxplot(data = Outlier_Free, x='Duration',y='price')\n",
    "plt.show()\n",
    "\n",
    "#For construction age\n",
    "Outlier_Free['CONSTRUCTION_AGE_BAND']=Outlier_Free['CONSTRUCTION_AGE_BAND'].map({ \n",
    "                                                               'England and Wales: 1930-1949': '1930-1949',\n",
    "                                                               'England and Wales: 1900-1929': '1900-1929',\n",
    "                                                               'England and Wales: 1950-1966': '1950-1966',\n",
    "                                                               'England and Wales: 1967-1975': '1967-1975',\n",
    "                                                               'England and Wales: before 1900': 'Before 1900',\n",
    "                                                               'England and Wales: 2003-2006': '2003-2006',\n",
    "                                                               'England and Wales: 1996-2002': '1996-2002',\n",
    "                                                               'England and Wales: 1976-1982': '1976-1982',\n",
    "                                                               'England and Wales: 1983-1990': '1983-1990',\n",
    "                                                               'England and Wales: 2007 onwards': '2007 onwards',\n",
    "                                                               'England and Wales: 1991-1995': '1991-1995'})\n",
    "plt.figure()\n",
    "ax = sns.boxplot(data = Outlier_Free, x='CONSTRUCTION_AGE_BAND',y='price')\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only the variables to be used in ML algorithm & Exploratory Data Analysis of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE FOR ITERATION 1 AND 2 ###\n",
    "Data = UsefulData[['price','Duration',\n",
    "                        'CURRENT_ENERGY_RATING', \n",
    "                        'propertyType', \n",
    "                        'TOTAL_FLOOR_AREA', \n",
    "                        'NUMBER_HABITABLE_ROOMS',\n",
    "                        'EXTENSION_COUNT',\n",
    "                        'CONSTRUCTION_AGE_BAND']]\n",
    "\n",
    "### USE FOR ITERATION 3 ###\n",
    "Data = Outlier_Free[['price','Duration',\n",
    "                        'CURRENT_ENERGY_RATING', \n",
    "                        'propertyType', \n",
    "                        'TOTAL_FLOOR_AREA', \n",
    "                        'NUMBER_HABITABLE_ROOMS',\n",
    "                        'EXTENSION_COUNT',\n",
    "                        'CONSTRUCTION_AGE_BAND',\n",
    "                        'Year']]\n",
    "\n",
    "### Use to create a heatmap of the numerical features\n",
    "For_Heatmap = Outlier_Free[['price',\n",
    "                        'TOTAL_FLOOR_AREA', \n",
    "                        'NUMBER_HABITABLE_ROOMS',\n",
    "                        'EXTENSION_COUNT',\n",
    "                        'Year']]\n",
    "\n",
    "#To plot Heatmap\n",
    "mask = np.triu(np.ones_like(For_Heatmap.corr(), dtype=np.bool))\n",
    "plt.figure()\n",
    "heatmap = sns.heatmap(For_Heatmap.corr(),mask=mask, vmin=-1, vmax=1, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);\n",
    "plt.show()\n",
    "#To see just correlations for price\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.set(font_scale=2)\n",
    "heatmap = sns.heatmap(For_Heatmap.corr()[['price']].sort_values(by='price', ascending=False), vmin=-1, vmax=1, annot=True,  annot_kws={\"size\":30})\n",
    "heatmap.set_title('Correlations Between Numerical Features & Sales Price', fontdict={'fontsize':20}, pad=16);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING: split dataset into features (X) and labels (y)\n",
    "X = Data.drop(columns=['price']) # the attributes\n",
    "y = Data.price #the label\n",
    "X.shape #(39491, 8)\n",
    "y.shape\n",
    "\n",
    "#PREPROCESSING: Covert ordinal categorical (the energy rating) to numerial\n",
    "X['CURRENT_ENERGY_RATING']=X['CURRENT_ENERGY_RATING'].map({'A': 6, 'B': 5,'C': 4, 'D': 3,'E': 2, 'F': 1,'G': 0})\n",
    "\n",
    "#PREPROCESSING: convert the rest of the categorical data to numerical forms. \n",
    "X = pd.get_dummies(X, drop_first= True)\n",
    "X.shape #(39491, 20)\n",
    "\n",
    "#(2454, 19) for iteration 1\n",
    "\n",
    "#Scaling using MinMax \n",
    "cols = X.columns\n",
    "X[cols] = MinMaxScaler().fit_transform(X[cols].values)\n",
    "X \n",
    "\n",
    "#PREPROCESSING: Train-test splitting\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "X_train.shape #(29618, 20)\n",
    "X_test.shape #(9873, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_ft_sel(model, feature_train, feature_test, label_train, seed):\n",
    "    \"\"\"Sequential Forward selection function\"\"\"\n",
    "    #Set 10-fold strafified cross validation\n",
    "    cv = KFold(n_splits=10, random_state = seed, shuffle = True) \n",
    "    #Set up sequential feature selector\n",
    "    sfs1 = SFS(estimator = model,\n",
    "            k_features = (1,20),\n",
    "            forward = True,\n",
    "            floating = False,\n",
    "            verbose = 2,\n",
    "            scoring = 'r2',\n",
    "            cv=cv,\n",
    "            n_jobs = -1)\n",
    "    #Fit the model and time it\n",
    "    tic_fwd = time()\n",
    "    sfs1 = sfs1.fit(feature_train,label_train)\n",
    "    toc_fwd = time()\n",
    "    #Return important information\n",
    "    print('\\n=======Time Taken=======')\n",
    "    time_taken = toc_fwd - tic_fwd\n",
    "    print(time_taken)\n",
    "    print('\\n=======Best score=======')\n",
    "    best_score = (sfs1.k_score_ )\n",
    "    print(best_score)\n",
    "    print('\\n=======Best Features=======')\n",
    "    best_features = sfs1.k_feature_names_ #features included\n",
    "    print(best_features)\n",
    "    print('\\n=======No. of Features=======')\n",
    "    no_features = len(best_features)\n",
    "    print(no_features)\n",
    "    #Plot features vs performance\n",
    "    plt.figure()\n",
    "    plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n",
    "    plt.title('Sequential Forward Selection (w. StdDev)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    #Make new subset based on selected features\n",
    "    feature_train_sfs = sfs1.transform(feature_train)\n",
    "    feature_test_sfs = sfs1.transform(feature_test)\n",
    "    #Return useful outputs\n",
    "    return(time_taken, best_score, best_features,feature_train_sfs,feature_test_sfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimisation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_optim(model, param_grid, feature_train_sfs, label_train):\n",
    "    \"\"\"Hyperparameter Optimisation Function\"\"\"\n",
    "    #set up grid search with parameter grid to be defined for each model\n",
    "    grid = GridSearchCV(model, param_grid, \n",
    "                        cv=10, \n",
    "                        scoring='r2', \n",
    "                        verbose = True, \n",
    "                        n_jobs = -1)\n",
    "    #Fit the model on train set transformed by slected features and time it\n",
    "    tic_fwd = time()\n",
    "    grid.fit(feature_train_sfs,label_train)\n",
    "    toc_fwd = time()\n",
    "    print('\\n=======Time Taken=======')\n",
    "    time_taken = toc_fwd - tic_fwd\n",
    "    print(time_taken)\n",
    "    print('\\n=======Best score=======')\n",
    "    best_score = (grid.best_score_)\n",
    "    print(best_score)\n",
    "    print('\\n=======Best Parameters=======')\n",
    "    best_parameters = grid.best_params_ #features included\n",
    "    print(best_parameters)\n",
    "    #Return useful outputs\n",
    "    return(time_taken, best_score, best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get metrics and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, feature_train_sfs, feature_test_sfs, label_train, label_test):\n",
    "    \"\"\"Function to fit model and return metrics & plots\"\"\"\n",
    "    model.fit(feature_train_sfs,label_train)\n",
    "    train_y_pred = model.predict(feature_train_sfs)\n",
    "    test_y_pred = model.predict(feature_test_sfs)\n",
    "    \n",
    "    train_y_pred = model.predict(feature_train_sfs)\n",
    "    test_y_pred = model.predict(feature_test_sfs)\n",
    "    \n",
    "    train_r2_score = metrics.r2_score(label_train, train_y_pred)\n",
    "    test_r2_score = metrics.r2_score(label_test, test_y_pred)\n",
    "    \n",
    "    train_mae_score = metrics.mean_absolute_error(label_train, train_y_pred)\n",
    "    test_mae_score = metrics.mean_absolute_error(label_test, test_y_pred)\n",
    "    \n",
    "    train_rmse_score = metrics.mean_squared_error(label_train, train_y_pred)\n",
    "    test_rmse_score = metrics.mean_squared_error(label_test, test_y_pred)\n",
    "    \n",
    "    residuals = (label_test - test_y_pred)\n",
    "\n",
    "    # Visualizing Our Training predictions\n",
    "    plt.figure()\n",
    "    plt.xlabel('Actual price')\n",
    "    plt.ylabel('Predicted Price') \n",
    "    plt.scatter(label_train,train_y_pred, s=1, c='black')\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "    plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    plt.title('Actual vs Predicred Price for Training Data')\n",
    "    # Perfect predictions\n",
    "    plt.plot(y_train,y_train,'r')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualizing Our Test predictions\n",
    "    plt.figure()\n",
    "    plt.xlabel('Actual price')\n",
    "    plt.ylabel('Predicted Price') \n",
    "    plt.scatter(label_test,test_y_pred, s=1, c='black')\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "    plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    plt.title('Actual vs Predicred Price for Test Data')\n",
    "    # Perfect predictions\n",
    "    plt.plot(y_test,y_test,'r')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualizing Our Test residuals\n",
    "    plt.figure()\n",
    "    plt.xlabel('Predicted price')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.scatter(label_test,residuals, s=1, c='black')\n",
    "    plt.title('Residuals Plot for Test Data')\n",
    "    # Perfect predictions\n",
    "    plt.axhline(y=0.5, color='r', linestyle='-')\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "    plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "    plt.show()\n",
    "    \n",
    "    return(train_r2_score, test_r2_score,\n",
    "           train_mae_score, test_mae_score,\n",
    "           train_rmse_score, test_rmse_score, residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "xgb_model = GradientBoostingRegressor(random_state = seed)\n",
    "#Feature selection\n",
    "xgb_fs_time_taken, xgb_fs_best_score, xgb_fs_best_features, xgb_X_train_sfs, xgb_X_test_sfs = fwd_ft_sel(xgb_model, X_train, \n",
    "                                                                                                         X_test, y_train, seed)\n",
    "\n",
    "#Hyperparameter Optimisation\n",
    "loss = ['squared_error', 'absolute_error']\n",
    "learning_rate_options = [0.1,0.2,0.3]\n",
    "xgb_param_grid = dict(loss = loss, \n",
    "                  learning_rate = learning_rate_options) \n",
    "xgb_ho_time_taken, xgb_ho_best_score, xgb_ho_best_parameters = hyperparam_optim(xgb_model, xgb_param_grid, xgb_X_train_sfs, y_train)\n",
    "\n",
    "#Model Metrics on train & test set\n",
    "xgb_op_model = GradientBoostingRegressor(random_state = seed, learning_rate = 0.3, loss = 'squared_error')\n",
    "xgb_train_r2_score, xgb_test_r2_score, xgb_train_mae_score, xgb_test_mae_score, xgb_train_rmse_score, xgb_test_rmse_score, xgb_residuals = get_metrics(xgb_op_model, xgb_X_train_sfs, xgb_X_test_sfs, y_train, y_test)\n",
    "\n",
    "#(0.7760370356050019,\n",
    "# 0.7578738359394868,\n",
    "# 22587.409672981692,\n",
    "# 23033.567369496333,\n",
    "# 986363596.2857233,\n",
    "# 1060213841.3433646)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "adb_model = AdaBoostRegressor(random_state = seed)\n",
    "#Feature selection\n",
    "adb_fs_time_taken, adb_fs_best_score, adb_fs_best_features, adb_X_train_sfs, adb_X_test_sfs = fwd_ft_sel(adb_model, X_train, \n",
    "                                                                                                         X_test, y_train, seed)\n",
    "#Hyperparameter Optimisation\n",
    "loss = ['linear', 'square', 'exponential']\n",
    "n_estimators_options = [10, 50, 100]\n",
    "adb_param_grid = dict(loss = loss, \n",
    "                  n_estimators = n_estimators_options) \n",
    "adb_ho_time_taken, adb_ho_best_score, adb_ho_best_parameters = hyperparam_optim(adb_model, adb_param_grid, adb_X_train_sfs, y_train)\n",
    "\n",
    "#Model Metrics on train & test set\n",
    "adb_op_model = AdaBoostRegressor(random_state = seed, loss = 'exponential', n_estimators = 10)\n",
    "adb_train_r2_score, adb_test_r2_score, adb_train_mae_score, adb_test_mae_score, adb_train_rmse_score, adb_test_rmse_score, adb_residuals = get_metrics(adb_op_model, adb_X_train_sfs, adb_X_test_sfs, y_train, y_test)\n",
    "\n",
    "#(0.6669311355459162,\n",
    "# 0.6648810301488107,\n",
    "# 28949.164508455662,\n",
    "# 28818.761585453984,\n",
    "# 1466880936.5030425,\n",
    "# 1467407587.7407577)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "rf_model = RandomForestRegressor(random_state = seed)\n",
    "#Feature selection\n",
    "rf_fs_time_taken, rf_fs_best_score, rf_fs_best_features, rf_X_train_sfs, rf_X_test_sfs = fwd_ft_sel(rf_model, X_train, X_test, y_train, seed)\n",
    "#Hyperparameter Optimisation\n",
    "n_estimators_options = [10, 50, 100]\n",
    "criterion_options = ['squared_error','absolute_error']\n",
    "rf_param_grid = dict(n_estimators = n_estimators_options, \n",
    "                  criterion = criterion_options) \n",
    "rf_ho_time_taken, rf_ho_best_score, rf_ho_best_parameters = hyperparam_optim(rf_model, rf_param_grid, rf_X_train_sfs, y_train)\n",
    "#Model Metrics on test set\n",
    "rf_op_model = RandomForestRegressor(random_state = seed, criterion = 'absolute_error', n_estimators = 100)\n",
    "rf_train_r2_score, rf_test_r2_score, rf_train_mae_score, rf_test_mae_score, rf_train_rmse_score, rf_test_rmse_score, rf_residuals = get_metrics(rf_op_model, rf_X_train_sfs, rf_X_test_sfs, y_train, y_test)\n",
    "\n",
    "#(0.9508126267114178,\n",
    "# 0.7217966563391365,\n",
    "# 10259.624264686023,\n",
    "# 24468.791757139967,\n",
    "# 217926613.56862903,\n",
    "# 1195910695.454584)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs to present results for Subset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit times\n",
    "adb_fit_time = adb_fs_time_taken + adb_ho_time_taken\n",
    "rf_fit_time = rf_fs_time_taken + rf_ho_time_taken\n",
    "xgb_fit_time = xgb_fs_time_taken + xgb_ho_time_taken\n",
    "\n",
    "# simple plot for fit time\n",
    "Fit_Times =  [adb_fit_time,rf_fit_time,xgb_fit_time]\n",
    "index = ['ADB','RF','XGB']\n",
    "Fit_Times_Results = pd.DataFrame({'Fit Time': Fit_Times}, index=index)\n",
    "\n",
    "plt.figure()\n",
    "Fit_Times_Results.plot.bar(rot=0, fontsize = 20, legend = None)\n",
    "plt.title('Fit Time For Each Model', fontsize = 20)\n",
    "plt.xlabel('Model', fontsize = 20)\n",
    "plt.ylabel('Fit Time (Seconds)', fontsize = 20)\n",
    "#plt.ylim(0.5,1.0)\n",
    "plt.show()\n",
    "\n",
    "###Stacked graph for computation times\n",
    "FS_times = [adb_fs_time_taken, rf_fs_time_taken, xgb_fs_time_taken]\n",
    "HypOp_times = [adb_ho_time_taken, rf_ho_time_taken, xgb_ho_time_taken]\n",
    "labels = index\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(labels, FS_times, label='Feature Selection')\n",
    "ax.bar(labels, HypOp_times, bottom=FS_times,\n",
    "       label='Model Specification')\n",
    "\n",
    "ax.set_ylabel('Computaton Time (Seconds)')\n",
    "ax.set_xlabel('Model')\n",
    "#ax.set_title('')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Plots of metric measures\n",
    "#r2\n",
    "r2_train =  [adb_train_r2_score,rf_train_r2_score,xgb_train_r2_score]\n",
    "r2_test =  [adb_test_r2_score,rf_test_r2_score,xgb_test_r2_score]\n",
    "index = ['ADB','RF','XGB']\n",
    "R2_Results = pd.DataFrame({'Train': r2_train,\n",
    "                           'Test' : r2_test }, index=index)\n",
    "\n",
    "#mae\n",
    "mae_train =  [adb_train_mae_score,rf_train_mae_score,xgb_train_mae_score]\n",
    "mae_test =  [adb_test_mae_score,rf_test_mae_score,xgb_test_mae_score]\n",
    "index = ['ADB','RF','XGB']\n",
    "MAE_Results = pd.DataFrame({'Train': mae_train,\n",
    "                            'Test': mae_test}, index=index)\n",
    "plt.figure()\n",
    "MAE_Results.plot.bar(rot=0, fontsize = 20)\n",
    "plt.title('MAE scores: Train vs Test Set', fontsize = 20)\n",
    "plt.xlabel('Model', fontsize = 20)\n",
    "plt.ylabel('MAE', fontsize = 20)\n",
    "plt.ylim(5000,32500)\n",
    "plt.show()\n",
    "\n",
    "#rmse\n",
    "rmse_train =  [adb_train_rmse_score,rf_train_rmse_score,xgb_train_rmse_score]\n",
    "rmse_test =  [adb_test_rmse_score,rf_test_rmse_score,xgb_test_rmse_score]\n",
    "index = ['ADB','RF','XGB']\n",
    "rmse_Results = pd.DataFrame({'Train': rmse_train,\n",
    "                             'Test': rmse_test}, index=index)\n",
    "plt.figure()\n",
    "rmse_Results.plot.bar(rot=0, fontsize = 20)\n",
    "plt.title('RMSE scores:Train vs Test Set', fontsize = 20)\n",
    "plt.xlabel('Model', fontsize = 20)\n",
    "plt.ylabel('RMSE', fontsize = 20)\n",
    "#plt.ylim(0.8,1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs to compare Subset 1,2 & 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comparing subset 1,2 & 3\n",
    "#Iteration 1\n",
    "Subset_1_train_r2 = 0.8208950689636683\n",
    "Subset_1_test_r2 = 0.32077186281664816\n",
    "\n",
    "#Iteration 2\n",
    "Subset_2_train_r2 = 0.4686699432202833\n",
    "Subset_2_test_r2 = 0.35621538902879553\n",
    "\n",
    "\n",
    "Subset_3_train_r2 = 0.776\n",
    "Subset_3_test_r2 = 0.758\n",
    "\n",
    "#Plots of r2 for GBM for each subset\n",
    "#r2\n",
    "subsets_r2_train =  [Subset_1_train_r2, Subset_2_train_r2 ,Subset_3_train_r2]\n",
    "subsets_r2_test =  [Subset_1_test_r2,Subset_2_test_r2,Subset_3_test_r2]\n",
    "index = ['1','2','3']\n",
    "Subset_R2_Results = pd.DataFrame({'Train': subsets_r2_train,\n",
    "                           'Test' : subsets_r2_test }, index=index)\n",
    "\n",
    "plt.figure()\n",
    "Subset_R2_Results.plot.bar(rot=0, fontsize = 15, colormap='RdGy')\n",
    "#plt.title('', fontsize = 20)\n",
    "plt.xlabel('Data Subset', fontsize = 15)\n",
    "plt.ylabel('R2 Score', fontsize = 15)\n",
    "#plt.ylim(0.5,1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
